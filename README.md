The architecture of the Dockerized AI Chatbot is structured to promote modularity, reusability, and ease of deployment. It follows a three-layered microservice-based architecture composed of a frontend user interface, a backend service, and an external AI model hosted via Hugging Face. The design focuses on a clear separation of concerns between user interaction, business logic, and natural language processing (NLP), which allows independent development, testing, and deployment of each component.

System Overview

The system is built using the following three major components:

Frontend (Streamlit): Provides a clean and interactive user interface for the end user to send and receive messages.

Backend (FastAPI): Receives user messages via API calls, forwards them to Hugging Face, and returns the AI-generated responses.

AI Model (Hugging Face): Processes the user query using the flan-T5-large transformer model and generates an appropriate response.

These components are packaged in isolated Docker containers, enabling seamless orchestration using Docker Compose.

Component-Level Architecture

A. Frontend:

Framework: Streamlit

Function: Accepts user input and displays the bot response

Role: Sends HTTP POST requests to the FastAPI backend

B. Backend:

Framework: FastAPI with Uvicorn ASGI server

Function: Hosts the /chat API endpoint, handles user requests, and connects to Hugging Face API

Includes: langgraph_workflow.py – handles request formatting, authentication, and API communication

C. AI Model:

Provided by: Hugging Face Inference API

Model: flan-t5-large (text-to-text generation)

Function: Accepts a prompt and returns a generated response in natural language

Data Flow and Interaction

Here’s a step-by-step breakdown of the data flow:

Step 1: User opens the chatbot in the browser (localhost:8501)

Step 2: User inputs a query into the Streamlit frontend

Step 3: Streamlit sends a POST request to FastAPI at http://localhost:8000/chat

Step 4: FastAPI extracts the input and passes it to the langgraph_workflow module

Step 5: langgraph_workflow formats the request and sends it to Hugging Face via HTTPS

Step 6: Hugging Face returns a text response generated by flan-T5

Step 7: The backend forwards the response to the frontend

Step 8: Streamlit displays the chatbot’s reply to the user

Design Considerations

Modularity: Frontend and backend are kept in separate folders, each with its own Dockerfile and requirements.txt for independent development.

Scalability: Although designed for local deployment, the microservice pattern and containerization make it easy to scale individual components.

Security: The API key is stored in a .env file and accessed using the dotenv module to avoid hardcoding sensitive credentials.

Maintainability: The codebase is organized into logical modules. For example, langgraph_workflow.py encapsulates all external API logic, making it reusable and easy to test.

Reproducibility: Docker ensures that the same environment can be recreated across different systems without conflicts.

Deployment Structure

The docker-compose.yml file defines both services:

backend:

build: ./backend

ports: 8000:8000

env_file: .env

frontend:

build: ./frontend

ports: 8501:8501

depends_on: [backend]

Docker Compose ensures that services start in the correct order and share the same network, enabling seamless communication between frontend and backend.

Visual Representation (Optional)

You can include a diagram like this:

User ↔ Streamlit (Frontend) → FastAPI (Backend) → Hugging Face API → AI Response → Back to User
